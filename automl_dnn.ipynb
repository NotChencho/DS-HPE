{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94824337",
   "metadata": {},
   "source": [
    "# installs imports paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598a4d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade scikit-learn\n",
    "!{sys.executable} -m pip install --upgrade pyarrow\n",
    "!{sys.executable} -m pip install --upgrade fastparquet\n",
    "!pip install matplotlib\n",
    "!pip install torch\n",
    "!{sys.executable} -m pip install scikit-learn --upgrade\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "path = r'C:\\Users\\julia\\OneDrive\\Desktop\\HPE'\n",
    "print(os.listdir(path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471dd77c",
   "metadata": {},
   "source": [
    "# automl - DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721aa881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from models.code_models.torch_models import DeepMLP\n",
    "\n",
    "BASE_PATH = \"C:/Users/julia/OneDrive/Desktop/HPE/DS-HPE\"\n",
    "\n",
    "month_map = {1: 5, 2: 6, 3: 7, 4: 8, 5: 9, 6: 10}\n",
    "\n",
    "\n",
    "def prepare_df(df):\n",
    "    df[\"dow\"] = pd.to_datetime(df[\"submit_time\"]).dt.dayofweek\n",
    "    df[\"dom\"] = pd.to_datetime(df[\"submit_time\"]).dt.day\n",
    "    df[\"hour\"] = pd.to_datetime(df[\"submit_time\"]).dt.hour\n",
    "    df[\"month\"] = pd.to_datetime(df[\"submit_time\"]).dt.month\n",
    "\n",
    "    df[\"is_weekend\"] = df[\"dow\"].isin([5, 6]).astype(int)\n",
    "    df[\"is_night\"] = ((df[\"hour\"] < 7) | (df[\"hour\"] >= 22)).astype(int)\n",
    "    df[\"is_peak\"] = df[\"hour\"].between(9, 18).astype(int)\n",
    "\n",
    "    df.drop(columns=[\"submit_time\"], inplace=True)\n",
    "\n",
    "    cols_to_drop = (\n",
    "        df.filter(like=\"dow_\").columns.tolist() +\n",
    "        df.filter(like=\"dom_\").columns.tolist() +\n",
    "        df.filter(like=\"hour_\").columns.tolist()\n",
    "    )\n",
    "    df.drop(columns=cols_to_drop, errors=\"ignore\", inplace=True)\n",
    "\n",
    "    for col in [\"group\", \"time_limit_cat\"]:\n",
    "        df[col] = df[col].astype(\"category\").cat.codes\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def evaluate_metrics(y_true, y_pred):\n",
    "    y_true = y_true.numpy()\n",
    "    y_pred = y_pred.numpy()\n",
    "    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2, axis=0))\n",
    "    mae = np.mean(np.abs(y_true - y_pred), axis=0)\n",
    "    r2 = 1 - np.sum((y_true - y_pred) ** 2, axis=0) / np.sum((y_true - np.mean(y_true, axis=0)) ** 2, axis=0)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-6)), axis=0) * 100\n",
    "    return {\"rmse\": rmse, \"mae\": mae, \"r2\": r2, \"mape\": mape}\n",
    "\n",
    "#use hyperparameters optimized during the sweeps\n",
    "def automl_torch_deepmlp(approach,\n",
    "                          batch_size=1028,\n",
    "                          dropout=0.15559998021833732,\n",
    "                          epochs=150,\n",
    "                          hidden_dims=[512, 256, 128],\n",
    "                          lr=0.0009719478103336032,\n",
    "                          patience=80,\n",
    "                          weight_decay=1e-5):\n",
    "\n",
    "    feature_cols = [\n",
    "        \"group\", \"num_tasks_final\", \"num_tasks_missing_or_inconsistent\",\n",
    "        \"time_limit_scaled\", \"time_limit_cat\", \"num_nodes_req\",\n",
    "        \"has_req_nodes\", \"num_cores_req\", \"cores_per_task\",\n",
    "        \"num_gpus_req\", \"mem_req\", \"has_req_threads_per_core\",\n",
    "        \"is_shared_job\", \"dow\", \"dom\", \"hour\", \"is_weekend\",\n",
    "        \"month\", \"is_night\", \"is_peak\"\n",
    "    ]\n",
    "    target_cols = [\"node_power_min\", \"node_power_mean\", \"node_power_max\"]\n",
    "\n",
    "    mape_history = {\"mean_power\": [], \"min_power\": [], \"max_power\": []}\n",
    "    iterations = []\n",
    "\n",
    "    scaler_X = StandardScaler()\n",
    "\n",
    "    for i in [1, 2, 3, 4, 5, 6]:\n",
    "        # load data from datasets corresponding to the approaches\n",
    "        if approach == \"accumulative\":\n",
    "            df = pd.read_csv(f\"{BASE_PATH}/my_dataframe{i}.csv\", low_memory=False)\n",
    "            df = prepare_df(df)\n",
    "            X = df[feature_cols].values\n",
    "            y = df[target_cols].values\n",
    "\n",
    "            X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "            X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "            X_train_full = np.vstack([X_train, X_val])\n",
    "            y_train_full = np.vstack([y_train, y_val])\n",
    "            iterations.append(f\"{i} months\")\n",
    "\n",
    "        elif approach == \"pairs\":\n",
    "            if i < 6:\n",
    "                df = pd.read_csv(f\"{BASE_PATH}/my_dataframe2{i}.csv\", low_memory=False)\n",
    "                df = prepare_df(df)\n",
    "                X = df[feature_cols].values\n",
    "                y = df[target_cols].values\n",
    "\n",
    "                X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "                X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "                X_train_full = np.vstack([X_train, X_val])\n",
    "                y_train_full = np.vstack([y_train, y_val])\n",
    "                iterations.append(f\"pair {month_map[i]}-{month_map[i+1]}\")\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "        elif approach == \"future-testing\":\n",
    "            if i < 6:\n",
    "                df_train = pd.read_csv(f\"{BASE_PATH}/my_dataframe{i}.csv\", low_memory=False)\n",
    "                df_test = pd.read_csv(f\"{BASE_PATH}/my_dataframe3{i+1}.csv\", low_memory=False)\n",
    "                iterations.append(f\"acc.{month_map[i]}-0{month_map[i+1]}\")\n",
    "            elif i == 6:\n",
    "                df_train = pd.read_csv(f\"{BASE_PATH}/my_dataframe{i}.csv\", low_memory=False)\n",
    "                df_test = pd.read_csv(f\"{BASE_PATH}/my_dataframe3{i}.csv\", low_memory=False)\n",
    "                iterations.append(f\"train-test: all acc.-0{month_map[i]}\")\n",
    "\n",
    "            df_train = prepare_df(df_train)\n",
    "            df_test = prepare_df(df_test)\n",
    "            X_train_full = df_train[feature_cols].values\n",
    "            y_train_full = df_train[target_cols].values\n",
    "            X_test = df_test[feature_cols].values\n",
    "            y_test = df_test[target_cols].values\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Unknown approach\")\n",
    "\n",
    "        # SCALING\n",
    "        X_train_full = scaler_X.fit_transform(X_train_full)\n",
    "        X_test = scaler_X.transform(X_test)\n",
    "\n",
    "        # turn the data into tensors\n",
    "        X_train_tensor = torch.tensor(X_train_full, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train_full, dtype=torch.float32)\n",
    "        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "        y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # define model, optimizer and criteria\n",
    "        model = DeepMLP(input_dim=X_train_full.shape[1], hidden_dims=hidden_dims, dropout=dropout)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        # early stopping set-up\n",
    "        best_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "\n",
    "        # TRAIN\n",
    "        model.train()\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            for xb, yb in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = model(xb)\n",
    "                loss = criterion(y_pred, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item() * xb.size(0)\n",
    "            epoch_loss /= len(train_loader.dataset)\n",
    "\n",
    "            # Early stopping\n",
    "            if epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                patience_counter = 0\n",
    "                best_model_state = model.state_dict()\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    model.load_state_dict(best_model_state)\n",
    "                    break\n",
    "\n",
    "        # evaluate\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred_test = model(X_test_tensor)\n",
    "        metrics = evaluate_metrics(y_test_tensor, y_pred_test)\n",
    "        #store the values of the mape for later plotting\n",
    "        mape_history[\"min_power\"].append(metrics[\"mape\"][0])\n",
    "        mape_history[\"mean_power\"].append(metrics[\"mape\"][1])\n",
    "        mape_history[\"max_power\"].append(metrics[\"mape\"][2])\n",
    "\n",
    "    # plot the MAPE evolution for each of the approaches\n",
    "    plt.figure()\n",
    "    plt.plot(iterations, mape_history[\"mean_power\"], marker=\"o\", label=\"Mean Power\")\n",
    "    plt.plot(iterations, mape_history[\"min_power\"], marker=\"o\", label=\"Min Power\")\n",
    "    plt.plot(iterations, mape_history[\"max_power\"], marker=\"o\", label=\"Max Power\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"MAPE (%)\")\n",
    "    plt.title(f\"MAPE Evolution - {approach}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6cd0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for approach in ['accumulative', 'pairs', 'future-testing']:\n",
    "    automl_torch_deepmlp(approach)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12",
   "language": "python",
   "name": "mi_kernel"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
