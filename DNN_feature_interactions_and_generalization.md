One of the main advantages of using a DNN over simpler linear models is its capability to capture complex non-linear relationships between the input features. The architecture with its progressive layer reduction (512→256→128 nodes) provides the model with the capacity to learn interactions between temporal features (is_weekend, is_night, is_peak) and resource allocation parameters (num_cores_req, num_gpus_req, mem_req), which is particularly relevant since power consumption patterns are likely to vary based on both when jobs run and what resources they demand. Linear models, by contrast, treat features independently unless interaction terms are explicitly engineered, which would require prior knowledge of which feature combinations are meaningful.

The most critical test of the model's practical utility comes from the future-testing approach, where we train on all available historical data and evaluate performance on completely unseen future months. After training on the accumulated data from May through September and testing on October data, the results show graceful degradation in performance. While in-distribution testing (accumulative and paired-month approaches) yields MAPE values that closely match validation metrics, typically within 1-2% difference, the future-testing scenario shows a moderate increase of approximately 3-7 percentage points. This degradation is within acceptable bounds for production deployment, as it indicates the model has learned fundamental relationships between job characteristics and power consumption rather than memorizing specific temporal patterns, with the conservative hyperparameter choices preventing overfitting to month-specific anomalies.